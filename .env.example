# GLHF Configuration
# API key for authentication with GLHF service
VITE_API_KEY_GLHF=your_glhf_api_key_here

# Base URL for GLHF's OpenAI-compatible API endpoint
VITE_API_URL_GLHF=https://glhf.chat/api/openai/v1

# Local proxy server URL for GLHF requests
VITE_BASE_URL_GLHF=http://localhost:3002/glhf

# Fast, lighter model for quick processing
VITE_AI_FAST_MODEL_GLHF="hf:meta-llama/Llama-3.2-1B-Instruct"

# More accurate but slower model for precise results
VITE_AI_PRECISE_MODEL_GLHF="hf:Qwen/Qwen2.5-Coder-32B-Instruct"

# LMStudio Configuration
# Local LMStudio server endpoint (OpenAI compatible)
VITE_API_URL_LMSTUDIO=http://localhost:1234/v1

# Model identifier for fast processing in LMStudio
VITE_AI_FAST_MODEL_LMSTUDIO="local-model"

# Model identifier for precise processing in LMStudio
VITE_AI_PRECISE_MODEL_LMSTUDIO="local-model"

# Local LM Configuration
# Ollama API endpoint for local model inference
VITE_API_URL_LOCAL_LM=http://localhost:11434/api/chat

# Fast model identifier for Ollama
VITE_AI_FAST_MODEL_LOCAL_LM="llama2"

# Precise model identifier for Ollama
VITE_AI_PRECISE_MODEL_LOCAL_LM="llama2"
